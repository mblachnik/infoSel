<?xml version="1.0" encoding="windows-1252" standalone="no"?>
<operatorHelp lang="en_EN">
    <!--  This is an example how to specify the translation of operator keys to names. -->
    <!--
    <operator>
      <key>example_operator_key</key>
      <name>Example Operator Name</name>
    </operator>
    -->

    <!-- This is how group keys are translated: -->
    <!--
    <group>
      <key>example_group</key>
      <name>Example Group</name>
    </group>
    -->

    <operator>
        <name>IS k-NN</name>
        <synopsis>k-NN classifier with extra functionality used by Instance Selection algorithms
        </synopsis>
        <help>A k nearest neighbor (k-NN) classifier.</help>
        <key>my_k_nn</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>LVQ Family</name>
        <synopsis>LVQ neural network</synopsis>
        <help>
            LVQ is a neural network which is based on local and competitive learning. 
            In this network neurons are called codebooks or prototypes. These codebooks 
            are returned on Prot output port, and they can be used as an input to the 
            nearest neighbor classifier.
             &lt;br/&gt; This operator implements standard LVQ algorithms described in: &lt;br/&gt; 
            Kohonen T., Self-Organizing Maps, Springer Verlag, 2001 &lt;br/&gt; 
            (They were implemented based on SOMPack) &lt;ul&gt; 
            &lt;li&gt; LVQ1 &lt;/li&gt; &lt;li&gt; LVQ2 &lt;/li&gt; &lt;li&gt; LVQ2.1 &lt;/li&gt; &lt;li&gt; LVQ3 &lt;/li&gt; &lt;li&gt; OLVQ &lt;/li&gt; &lt;/ul&gt; 
            This operator also implements more complex LVQ networks such as: &lt;ul&gt; &lt;li&gt; WLVQ - Weighted LVQ in details described in: &lt;br/&gt; Blachnik M., Duch W., LVQ algorithm with instance weighting for generation of prototype-based rules, Neural Networks vol 24(8), Elsevir, pp. 824-830 2011 &lt;/br&gt; It requires weight attribute, which is taken as a source of weights. Weights can be obtained from ENN Weighting operator &lt;/li&gt; &lt;li&gt; SLVQ - a concept of semi supervised LVQ algorithm - very naive implementation which behaves as VQ clustering when labels are not present (NaNs) and if labels are present it performs LVQ update rule&lt;/li&gt; &lt;li&gt; WTM_LVQ - Winer takes most LVQ implementation instead of Winer takes all. If EUCLIDIAN neighborhood is selected then all codebooks which fall into a hyper-sphere defined by the closest prototype and lambda radius are updated using standard LVQ formula.&lt;/br&gt; If GAUSSIAN neighborhood is selected then the neighborhood has vanishing radius defined as g = exp(- (dist^2) / (2 * lambda^2)) and the update rule is defined as {prototype = prototype +/- alpha*g* (example - prototype)} &lt;/li&gt; &lt;li&gt; SNG - Supervised Stochastic Neural Gas. This algorithm is similar to the WTM_LVQ algorithm with GAUSSIAN neighborhood except that instead of directly taking distances between prototypes an order of the distances is considered (1,2,3,4) so {g=exp(- j/lambda) &lt;br/&gt; For details see: &lt;br/&gt; Villmann, Thomas, Barbara Hammer, and Marc Strickert. &quot;Supervised neural gas for learning vector quantization.&quot; Proc. of the 5th German Workshop on Artificial Life (GWAL-5). 2002. &lt;/li&gt; &lt;li&gt; GLVQ - generalized LVQ algorithm. It uses well defined cost function for which gradient can be explicit calculated &lt;br/&gt; For details see: &lt;/br/&gt; Hammer, Barbara, and Thomas Villmann. &quot;Generalized relevance learning vector quantization.&quot; Neural Networks 15.8 (2002): 1059-1068. &lt;/li&gt; &lt;/ul&gt;
            <!--
            This operator implements a set of different LVQ algorithms. It can be used for instance optimization for nearest neighbor classifier. &lt;br/&gt; It implements: &lt;ul&gt; &lt;li&gt; LVQ1 &lt;/li&gt; &lt;li&gt; LVQ2 &lt;/li&gt; &lt;li&gt; LVQ2.1 &lt;/li&gt; &lt;li&gt; LVQ3 &lt;/li&gt; &lt;li&gt; OLVQ &lt;br/&gt; For details of these for algorithm look at &lt;br/&gt; Kohonen T., Self-Organizing Maps, Springer Verlag, 2001 &lt;br/&gt; These are java versions of algorithms implemented in SOMPack &lt;/li&gt; &lt;li&gt; WLVQ &ndash; Weighted LVQ in details described in: &lt;br/&gt; Blachnik M., Duch W., LVQ algorithm with instance weighting for generation of prototype-based rules, Neural Networks vol 24(8), Elsevir, pp. 824-830 2011 &lt;/br&gt; It requires weight attribute, which is taken as a source of weights. Weights can be obtained from ENN Weighting operator &lt;/li&gt; &lt;/ul&gt;
            -->
        </help>
                
        <key>lvq</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>VQ</name>
        <synopsis>Vector Quantization clustering algorithm</synopsis>
        <help>
            This operator implements Vector Quantization (VQ) clustering algorithm. &lt;br/&gt;
            It is a kind of neural network learned on Winner Takes All principle using stochastic gradient algorithm.
            Neurons of this network are called codebooks, and each codebook represent one cluster center.
            As a result this neural network returns a set of codebooks position as an ExampleSet. It also offers 
            a model which could be used for clustering new incoming data samples.
            It can be used for instance optimization and clustering. 
                &lt;br/&gt;
            For details see: 
                &lt;br/&gt;
            Kohonen T., Self-Organizing Maps, Springer Verlag, 2001                                 
        </help>                
        <key>vq</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>FlattenClustersByDistance</key>
        <class>com.rapidminer.ispr.operator.clustering.FlattenByDistanceClusterModel</class>        
        <name>Flatten Clusters By Distance</name>
        <synopsis>Flatten Clusters by Distance value</synopsis>
        <help>This algorithm can be used as an alternative for default Flatten Clustering algorithm provided by RapidMiner.
            It is used to split data clustered with Agglomerative Clustering into clusters. &lt;br/&gt;
            Instead of manually setting the number of clusters this operator allows to define a threshold on the dendrogram height.
            It can be interpreted as a maximum allowed distance between different clusters. This could be visualized using dendrogram
            view of the Agglomerative Clustering operator as some cut-point level on which the dendrogram is catted. Unfortunately 
            the default RapidMiner Agglomerative Clustering operator don't provide a scale on the vertical axes of the dendrogram view.
        </help>
        <icon>ispr.png</icon>
    </operator>
    <!--
    <operator>
        <name>Flatten Clusters By Distance</name>
        <synopsis>Flatten Clusters By Distance</synopsis>
        <help>Similar to Flatten Clustering it cuts the dendrogram but instead of setting the number of clusters here it is based on setting the similarity threshold between clusters</help>
        <key>FlattenClustersByDistance</key>
    </operator>
    -->
    <operator>
        <name>Random selection</name>
        <synopsis>Random instances selection</synopsis>
        <help>
            This operator performs naive example set filtering. It simple randomly selects examples.
            It is very similar to Sample operator.
        </help>
        <key>random_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>CNN selection</name>
        <synopsis>Condensed Neares Neighbor instance selection</synopsis>
        <help>
            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Hart P.E., The condensed nearest neighbor rule., IEEE Trans. on Information Theory, vol. 16, pp. 515-516, 1968 
        </help>
        <key>cnn_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>IB2 selection</name>
        <synopsis>IB2 instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Acha D., Kibler D., Albert M, Instance-Based Learning Algorithms., Machine Learning, vol.6, pp. 37-66, 1991
        </help>
        <key>ib2_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>IB3 selection</name>
        <synopsis>IB3 instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Acha D., Kibler D., Albert M, Instance-Based Learning Algorithms., Machine Learning, vol.6, pp. 37-66, 1991
        </help>
        <key>ib3_sel</key>
        <icon>ispr.png</icon>
    </operator>
        
    <operator>
        <name>ENN selection</name>
        <synopsis>Edited Neares Neighbor instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Wilson D.L., Assymptotic properties of nearest neighbour rules using edited data., IEEE Trans. on Systems, Man, and Cybernetics, Computational Intelligence, SMC-2, pp. 408-421, 1972
        </help>
        <key>enn_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>RENN selection</name>
        <synopsis>Repeated Edited Neares Neighbor instances selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Tomek I., An experiment with the edited nearest-neighbor rule., IEEE Trans. on Systems, Man, and Cybernetics, vol. 6, pp. 448-452, 1976
        </help>
        <key>renn_sel</key>
        <icon>ispr.png</icon>
    </operator>
  
    <operator>
        <name>All-kNN selection</name>
        <synopsis>All kNN instances selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Tomek I., An experiment with the edited nearest-neighbor rule., IEEE Trans. on Systems, Man, and Cybernetics, vol. 6, pp. 448-452, 1976
        </help>
        <key>all_knn_sel</key>
        <icon>ispr.png</icon>
    </operator>
	
    <operator>
        <name>GE selection</name>
        <synopsis>Instance selection based on Gabriel Graph Editing. This algorithm keeps instances which are on the border between different classes according to Gabriel Graph </synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Binay K. Bhattacharya, Ronald S. Poulsen, Godfried T. Toussaint,  "Application of Proximity Graphs to Editing Nearest Neighbor Decision Rule",  International Symposium on Information Theory, Santa Monica, 1981.
            &lt;br/&gt;            
        </help>
        <key>ge_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>RNG selection</name>
        <synopsis>Instance selection based on Relative Neighbor Graph Editing This algorithm keeps instances which are on the border between different classes</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Godfried T. Toussaint, The Relative Neighborhood Graph of a Finite Planar Set, Pattern Recognition, vol.12, No.4, pp.261-268, 1980
            &lt;br/&gt;            
        </help>
        <key>rng_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>RMHC selection</name>
        <synopsis>Instance selection based on Random Mutation Hill Climbing </synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
            Skalak, David B. "Prototype and feature selection by sampling and random mutation hill climbing algorithms." Proceedings of the eleventh international conference on machine learning. 1994.
        &lt;br/&gt;
            
        </help>
        <key>rmhc_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>MC selection</name>
        <synopsis>Instance selection based on Monte Carlo sampling </synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;                        
        </help>
        <key>mc_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <!--
<operator>
    <name>IB3 selection</name>
    <synopsis>IB3 instances selection</synopsis>
    <help>IB3 selection</help>
    <key>ib3_sel</key>
</operator>  
    -->
    <operator>
        <name>ELH selection</name>
        <synopsis>ELH instances selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
            &lt;br/&gt;
            Cameron-Jones, R. M. "Instance selection by encoding length heuristic with random mutation hill climbing." Eighth Australian Joint Conference on Artificial Intelligence. 1995.
        </help>
        <key>elh_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Weka IS selection</name>
        <key>weka_sel</key>
        <synopsis>Weka instances selection</synopsis>
        <help>             This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            This operator is a wrapper over IS Library developed for Weka, and was implemented by Dr. Alvar Arnaiz-Gonzalez from University of Burgos
        </help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Generalized ENN</name>
        <synopsis>GENN instances selection</synopsis>
        <help>It is an implementation of Generalized ENN algorithm. It allows for replacing default kNN classifier by
            any other classification algorithm. By doing this user can tune training set for given particular classifier.
            By doing this all noisy examples will be filtered. However it may be very time consuming as it performs leave one out principle to perform such analysis 
        </help>
        <key>genn_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Generalized CNN</name>
        <synopsis>Generalized Condensed Neares Neighbor instance selection</synopsis>
        <help>It is an implementation of Generalized CNN algorithm. It allows for replacing default kNN classifier by
            any other classification algorithm. By doing this user can tune training set for given particular classifier.
                                    
        </help>
        <key>gcnn_sel</key>
        <icon>ispr.png</icon>
    </operator>
  
    <operator>
        <name>Infosel feature ranker</name>
        <synopsis>Infosel feature ranker</synopsis>
        <help>Feature selection based on Infosel++ library, for detaitls see: www.prules.org. This library was written in C++, and this operator is 
            a wrapper for that library. It requires to define name of the feature selection algorithm and its parameters. 
            For estimation of probability dystribution it requires discrete values so the input set discretised by this library with one
            of build in methods. All the parameters are given as strings because this allows to keep this library open for new algorithms 
            implemented init. &lt;/br&gt;
            Remember that this RapidMiner plugin requires to have infosel++.exe. It is not included in this extension. You have to ask for it Jacek Biesiada who recently works at  Cincinnati Children's Hospital Medical Center.
            &lt;/br&gt;
            Then you have configure this operator by pointing the folder which include infosel++.exe library.
        </help>
        <key>infosel_featsel</key>
        <icon>ispr.png</icon>
    </operator>
    <!--
    <operator>
        <name>Feature weights transformer</name>
        <synopsis>Feature weights transformer</synopsis>
        <help>This operator is used to transform feature weights. After passing this transformer features which should be present 
            have weights equals 1, and features which should be removed have weights equal 0. 
        &lt;br/&gt;
            It was designed because Infosel++ returns feature order, such that 
            each feature has assigned natural number, and this number represents its importance. However these numbers are whithout 
            any information if they are ordered from the most relevant to the least one or in the oposit order. The order 
            depends on the algorithm. For example mi_mi retunrs higher values for more relevant features.                
        </help>
        <key>featsel_weightsmodifier</key>
    </operator>
    -->
    <!--
    <operator>
          <name>Feature Selection Operator</name>
          <synopsis>Feature Selection Operator</synopsis>
          <help>Feature Selection Operator</help>
          <class>com.rapidminer.ispr.operator.learner.feature.selection.FeatureSelectionOperator</class>
          <key>featsel_model</key>
    </operator>
    -->
    <operator>
        <name>FCM</name>
        <synopsis>Fuzzy c-Means </synopsis>
        <help>
            Fuzzy c-means clustering algorithm. As a result it aslo returns cluster centers on Proto output port. &lt;br/&gt;
            For details of this algorithm see: 
        &lt;br/&gt;        
            Bezdek J.C., Pattern Recognition with Fuzzy Objective Function Algorithms, Plenum, New York, 1981 
        </help>
        <class>com.rapidminer.ispr.operator.learner.optimization.FCMOperator</class>
        <key>FCM</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>CFCM</name>
        <synopsis>Conditional Fuzzy c-Means </synopsis>
        <help>Conditional Fuzzy c-means clustering algorithm. This algorithm requires weight attribute which is used as a clustering condition.
            As a result it aslo returns cluster centers on Proto output port. &lt;br/&gt;
            For details of this algorithm see: 
        &lt;br/&gt;
            Pedrycz W., Conditional Fuzzy C-Means, Pattern Recognition Letters, Vol. 17(6), 1996
        </help>
        <class>com.rapidminer.ispr.operator.learner.optimization.CFCMOperator</class>
        <key>CFCM</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Class assigner</name>
        <synopsis>Assigns appropriate class label for a set of prototype instances.</synopsis>
        <help>This operator determine class label for each instance delivered to the Prototypes input based 
            on the frequency of labels of the nearest instances from the ExampleSet input. 
        &lt;br/&gt;
            Usually it is used for assigning label to the prototypes (cluster centers) obtained after clustering some labeled exampleset.            
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.ClassAssignerOperator</class>
        <key>ClassAssigner</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Class iterator</name>
        <synopsis>Iterates over each class label of an example set </synopsis>
        <help> This operator iterates over each value of class label of input example set, allowing to applay 
            instance processing method (ex. clastering) to each class independent. Finally all example sets delivered 
            to the output are combined such that the output Prototypes containes concatenation of all instances delivered to the internal output.
            If attribute subset is not consistent then output set contains all possible attributes, and missing values are set to NaN.
            &lt;br/&gt;
            For example it can be used to assign class labels to prototypes (cluster centers) 
            obtained as a result of clustering algorithm, or
            when having a labeled dataset and someone wants to cluster examples separately for each class label. (labels must be nominal)          
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.ClassIteratorOperator</class>
        <key>ClassIterator</key>
        <icon>ispr.png</icon>
    </operator>  
    
    <operator>
        <name>ENN Weighting</name>
        <synopsis>Instance Weighting</synopsis>
        <help>    
            This algorithm performs ENN algorithm but instead of selecting instances it assignes weight to each instance. 
            This weight is in range [0,1], and represent the value which can be interpreted as a probability of not being an outlier or noise sample.
            Among other usage, it can be used to train WLVQ network.
        </help>
        <class>com.rapidminer.ispr.operator.learner.weighting.ENNWeightingOperator</class>
        <key>ENNWeighting</key>
        <icon>ispr.png</icon>
    </operator> 
    <!--
    <operator>
        <name>Weight Transformation</name>
        <synopsis>Weight Transformation</synopsis>
        <help> 
        </help>
        <class>com.rapidminer.ispr.operator.learner.weighting.AttributeTransformation</class>
        <key>WeightTransformation</key>
    </operator> 
    -->
    <!--
    <operator>
        <name>Values Difference</name>
        <synopsis>For selected attributes it calculates a difference between vollowing neighbor values</synopsis>
        <help> 
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.ValueDifferenceOperator</class>
        <key>ValueDifference</key>
    </operator>       
    <operator>
        <name>Values Shift</name>
        <synopsis>For selected attributes it shifts values from previous to new position</synopsis>
        <help> 
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.ValuesShiftOperator</class>
        <key>ValuesShift</key>
    </operator>       
    -->
    <operator>
        <name>Export Process</name>
        <synopsis>Export the process to a file or a repository</synopsis>
        <help> 
            This operator makes a copy of the xml process description to a given location. It is a specialy usefull when you run multiple experiments with different modifications, so you can store not only the results log but automatically the corresponding process description. So it is simmilar to Auto-Save As where you can specify the location of the process.
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.ProcessExportOperator</class>                
        <key>ExportProcess</key>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <name>Group Distance</name>
        <synopsis>Calculate the distance between group of examples</synopsis>
        <help> 
            This operator splits the input exampleSet according to the "grouping attribute", then it calculates the distance between each of the groups. The
            distance between groups is calculated as the average of the distance between two closes examples in each group. The returned exampleset is a square matrix 
            however it is not symetric, because it is calculated ina way where it takes each exampe from one group and finds closest example in the second group. So it
            is possible that group A will be mre similar to group B then group B is similar to group A. For example it may happen when examples in group A are a
            subset of examples in group B.
        </help>
        <class>com.rapidminer.ispr.operator.learner.misc.GroupDistanceOperator</class>                
        <key>GroupDistance</key>
        <icon>ispr.png</icon>
    </operator> 
    <operator>
        <synopsis>Delivers as logging value an input object ID</synopsis>
        <help> 
            This operator can be used to provide as a log value the ID value of the input IOObject (this could be ExampleSet or Prediction mMopdel or any other operator).
            
            In can be used to identify the model, which returned certain results. For example when using X-Validation (parallel )  it is impossible to combine results logged on the training and test side. By using prediction model ID you can collect logs independently on the training side and testing side and then join them together.
        </help>
        <name>Get ID</name>
        <key>InputID</key>
        <icon>ispr.png</icon>
        <class>com.rapidminer.ispr.operator.learner.misc.GetIOModelID</class>                
    </operator> 
    <operator>
        <key>is_performance</key>
        <class>com.rapidminer.ispr.operator.performance.InstanceSSelectionPerformance</class>
        <name>IS Performance</name>
        <synopsis>Calculate various IS performances</synopsis>
        <help> 
            This operator gets two ExampleSet's as input and calculates various statistics related to the instance selection problems. These are size_difference - a difference between sizes of both example sets (the value is abs), relative_size_difference - size_difference divided by the size of the first example set, compression - calculated as the ration of example set sizes
        </help>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <key>cluster_performance</key>
        <class>com.rapidminer.ispr.operator.performance.ClusteringMinimumVarianceCriterion</class>
        <name>Cluster Performance</name>
        <synopsis>Calculate within cluster variance and returns it as performance</synopsis>
        <help> 
            This operator is used to analyze the performance of the clustering methods. It gets a set of cluster prototypes and  ExampleSet's as input, and calculates the variance within clusters - the same as is used in k-means clustering
        </help>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <key>bagging_sel</key>
        <name>Bagging IS</name>
        <class>com.rapidminer.ispr.operator.learner.selection.meta.ISMetaBaggingOperator</class>
        <synopsis>Meta Instance Selection based on Bagging</synopsis>
        <help>This operator implements bagging instance selection. It repeates instance selectin process given number of times and for each example calculates average number of times example was selected. This average may be grater then 1 because it uses sampling with replacement so it is possible that single vector may be selected more often then the number of iterations. The number of times given vector was selected depends on the ID in the output set of the subprocess. So if all vectors will be delivered to the output all vectors will be selected (according to the bootstrap distribution)</help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>vote_sel</key>
        <name>Voted IS</name>
        <class>com.rapidminer.ispr.operator.learner.selection.meta.ISMetaVoteOperator</class>
        <synopsis>Meta Instance Selection based on Voting</synopsis>
        <help>It allows to combine results of many independent instance selection algorithms. It combines selected instances and calculate votes, how often given example was selected by all of the instance selection methods. Finally instances selected more often then the threshold will appear in the final selected example set.</help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>RndFeature_sel</key>
        <name>Meta - Random Features IS</name>
        <class>com.rapidminer.ispr.operator.learner.selection.meta.ISMetaRndFeatureOperator</class>
        <synopsis>Meta Instance Selection based on Random Features</synopsis>
        <help>
            This meta operator for instance selection utilize the random features principle to support divergence of the model.
            This algorithm repeats instance selection process, and in each iteration a subset of features (ratio defines the percentage of features to be select) is selected  and delivered to the input of the process. According to the selected instances  in each iteration a ranking is made, such that the weight represents the number of times given instance was returned by the instance selection subprocess. The threshold parameter allows to manipulate the popularity of selected instances. If the "return weight" option is selected instead of selecting the instances a weight attribute is returned. As in input to that process this operator requires ExampleSet with ID attribute.
        </help>
        <icon>ispr.png</icon>
    </operator>   
        
    <operator>
        <key>meta_noise_sel</key>
        <name>Meta - Nois IS</name>
        <class>com.rapidminer.ispr.operator.learner.selection.meta.ISMetaNoisOperator</class>
        <synopsis>Meta Instance Selection based on additive Gaussian Noise</synopsis>
        <help>      
            This meta operator for instance selection utilize the random features principle to support divergence of the model.
            This algorithm repeats instance selection process, and in each iteration a subset of features (ratio defines the percentage of features to be select) is selected  and delivered to the input of the process. According to the selected instances  in each iteration a ranking is made, such that the weight represents the number of times given instance was returned by the instance selection subprocess. The threshold parameter allows to manipulate the popularity of selected instances. If the "return weight" option is selected instead of selecting the instances a weight attribute is returned. As in input to that process this operator requires ExampleSet with ID attribute.
        </help>
        <icon>ispr.png</icon>
    </operator>   
    
    <key>mds_scaling</key>
    <class>com.rapidminer.ispr.operator.learner.feature.construction.MdsOperator</class>
				
    <operator>
        <key>mds_scaling</key>
        <name>MDS Operator</name>
        <synopsis>This operator uses special MDS algorithm from MDSJ library to provide a visual
            representation of the pattern of proximities (i.e., similarities or
            distances) among a set of objects.
        </synopsis>
        <help>An MDS algorithm aims to place an example defined in high dimensional space into N-dimensional space (usually N=2) such that the between-object distances in high dimensional (origininal)  space and low dimensional space are preserved as accurate as possible.&lt;br&gt; Note that MDSJ library used by the operator is licenced under the Creative Commons License &quot;by-nc-sa&quot; 3.0 regulations. This means that you can: &lt;ul&gt; &lt;li&gt;freely copy, share, and distribute the library at no cost and &lt;/li&gt; &lt;li&gt;re-use the library as a component in your software,&lt;/li&gt; &lt;/ul&gt; as long as you: &lt;ul&gt; &lt;li&gt; include the citation below (by), &lt;/li&gt; &lt;li&gt; do not use MDSJ for any commercial purposes (nc), and &lt;/li&gt; &lt;li&gt; apply these conditions to all your pieces of software that make use of MDSJ (sa).&lt;/li&gt; &lt;/ul&gt; . &lt;br&gt; Citation: &quot;Algorithmics Group. MDSJ: Java Library for Multidimensional Scaling (Version 0.2). Available at http://www.inf.uni-konstanz.de/algo/software/mdsj/. University of Konstanz, 2009.&quot;
        </help>
        <icon>ispr.png</icon>
    </operator>

    <!-- This is how group keys are translated: -->
    <group>
        <key>Feature_reduction</key>
        <name>Feature reduction</name>
    </group>
</operatorHelp>
