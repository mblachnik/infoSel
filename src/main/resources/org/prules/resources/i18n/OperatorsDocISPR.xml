<?xml version="1.0" encoding="windows-1252" standalone="no"?>
<operatorHelp lang="en_EN">
    <!--  This is an example how to specify the translation of operator keys to names. -->
    <!--
    <operator>
      <key>example_operator_key</key>
      <name>Example Operator Name</name>
    </operator>
    -->

    <!-- This is how group keys are translated: -->
    <!--
    <group>
      <key>example_group</key>
      <name>Example Group</name>
    </group>
    -->

    <operator>
        <name>k-NN IS</name>
        <synopsis>k-NN classifier with extra functionality used by Information Selection algorithms
        </synopsis>
        <help>A k nearest neighbor (k-NN) classifier.</help>
        <key>my_k_nn</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>NeuralNet LVQ</name>
        <synopsis>LVQ neural network</synopsis>
        <help>
            LVQ is a neural network which is based on local and competitive learning. 
            In this network neurons are called codebooks or prototypes. These codebooks 
            are returned on Prot output port, and they can be used as an input to the 
            nearest neighbor classifier.
             &lt;br/&gt; This operator implements standard LVQ algorithms described in: &lt;br/&gt; 
            Kohonen T., Self-Organizing Maps, Springer Verlag, 2001 &lt;br/&gt; 
            (They were implemented based on SOMPack) &lt;ul&gt; 
            &lt;li&gt; LVQ1 &lt;/li&gt; &lt;li&gt; LVQ2 &lt;/li&gt; &lt;li&gt; LVQ2.1 &lt;/li&gt; &lt;li&gt; LVQ3 &lt;/li&gt; &lt;li&gt; OLVQ &lt;/li&gt; &lt;/ul&gt; 
            This operator also implements more complex LVQ networks such as: &lt;ul&gt; &lt;li&gt; WLVQ - Weighted LVQ in details described in: &lt;br/&gt; Blachnik M., Duch W., LVQ algorithm with instance weighting for generation of prototype-based rules, Neural Networks vol 24(8), Elsevir, pp. 824-830 2011 &lt;/br&gt; It requires weight attribute, which is taken as a source of weights. Weights can be obtained from ENN Weighting operator &lt;/li&gt; &lt;li&gt; SLVQ - a concept of semi supervised LVQ algorithm - very naive implementation which behaves as VQ clustering when labels are not present (NaNs) and if labels are present it performs LVQ update rule&lt;/li&gt; &lt;li&gt; WTM_LVQ - Winer takes most LVQ implementation instead of Winer takes all. If EUCLIDIAN neighborhood is selected then all codebooks which fall into a hyper-sphere defined by the closest prototype and lambda radius are updated using standard LVQ formula.&lt;/br&gt; If GAUSSIAN neighborhood is selected then the neighborhood has vanishing radius defined as g = exp(- (dist^2) / (2 * lambda^2)) and the update rule is defined as {prototype = prototype +/- alpha*g* (example - prototype)} &lt;/li&gt; &lt;li&gt; SNG - Supervised Stochastic Neural Gas. This algorithm is similar to the WTM_LVQ algorithm with GAUSSIAN neighborhood except that instead of directly taking distances between prototypes an order of the distances is considered (1,2,3,4) so {g=exp(- j/lambda) &lt;br/&gt; For details see: &lt;br/&gt; Villmann, Thomas, Barbara Hammer, and Marc Strickert. &quot;Supervised neural gas for learning vector quantization.&quot; Proc. of the 5th German Workshop on Artificial Life (GWAL-5). 2002. &lt;/li&gt; &lt;li&gt; GLVQ - generalized LVQ algorithm. It uses well defined cost function for which gradient can be explicit calculated &lt;br/&gt; For details see: &lt;/br/&gt; Hammer, Barbara, and Thomas Villmann. &quot;Generalized relevance learning vector quantization.&quot; Neural Networks 15.8 (2002): 1059-1068. &lt;/li&gt; &lt;/ul&gt;
            <!--
            This operator implements a set of different LVQ algorithms. It can be used for instance optimization for nearest neighbor classifier. &lt;br/&gt; It implements: &lt;ul&gt; &lt;li&gt; LVQ1 &lt;/li&gt; &lt;li&gt; LVQ2 &lt;/li&gt; &lt;li&gt; LVQ2.1 &lt;/li&gt; &lt;li&gt; LVQ3 &lt;/li&gt; &lt;li&gt; OLVQ &lt;br/&gt; For details of these for algorithm look at &lt;br/&gt; Kohonen T., Self-Organizing Maps, Springer Verlag, 2001 &lt;br/&gt; These are java versions of algorithms implemented in SOMPack &lt;/li&gt; &lt;li&gt; WLVQ &ndash; Weighted LVQ in details described in: &lt;br/&gt; Blachnik M., Duch W., LVQ algorithm with instance weighting for generation of prototype-based rules, Neural Networks vol 24(8), Elsevir, pp. 824-830 2011 &lt;/br&gt; It requires weight attribute, which is taken as a source of weights. Weights can be obtained from ENN Weighting operator &lt;/li&gt; &lt;/ul&gt;
            -->
        </help>
                
        <key>lvq</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Vector Quantization (VQ)</name>
        <synopsis>Vector Quantization clustering algorithm (VQ)</synopsis>
        <help>
            This operator implements Vector Quantization (VQ) clustering algorithm. &lt;br/&gt;
            It is a kind of neural network learned on Winner Takes All principle using stochastic gradient algorithm.
            Neurons of this network are called codebooks, and each codebook represent one cluster center.
            As a result this neural network returns a set of codebooks position as an ExampleSet. It also offers 
            a model which could be used for clustering new incoming data samples.
            It can be used for instance optimization and clustering. 
                &lt;br/&gt;
            For details see: 
                &lt;br/&gt;
            Kohonen T., Self-Organizing Maps, Springer Verlag, 2001                                 
        </help>                
        <key>VQ</key>
        <icon>ispr.png</icon>
        
    </operator>
    <operator>
        <key>NearestPrototypesOperator</key>
        <name>Nearest Prototypes Batch</name>
        <synopsis>Generates batch attribute out of two nearest prototypes from oposite classes</synopsis>
        <help>
            For each training example it finds two closest prototypes from oposite classes, and assignes unique identifier for the pair. This identifier constitutes a batch attribute
        </help> 
        <icon>ispr.png</icon>
    </operator> 
    
    <operator>
        <key>BatchLoopOperator</key>
        <name>Loop Batches and Build Ensemble Operator</name>
        <synopsis>Loop over batches in example set and for each batch trains an independant model. Then all models are combined into EnsemblePredictionModel.</synopsis>
        <help>
            
        </help> 
        <icon>ispr.png</icon>
    </operator> 
            
    <operator>
        <key>NNE_by_D_test</key>
        <name>Performance D-test</name>
        <synopsis>Nonparametric Residual Noise Estimation Delta-test</synopsis>
        <help>
            
        </help> 
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <key>NNE_by_G_test</key>
        <name>Performance G-test</name>
        <synopsis>Nonparametric Residual Noise Estimation Gamma-test</synopsis>
        <help>
            
        </help>         
        <icon>ispr.png</icon>
    </operator>    
        
    <operator>
        <key>weightBy_NNE_D_test</key>
        <name>Estimate Noise D-test</name>
        <synopsis>Nonparametric Residual Noise Estimation per example using Delta-test</synopsis>
        <help>
            
        </help>         
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <key>weightBy_NNE_G_test</key>
        <name>Estimate Noise (G-test)</name>
        <synopsis>Nonparametric Residual Noise Estimation per example using Gamma-test</synopsis>
        <help>            
        </help>         
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>weightBy_Local_NNE_G_test</key>
        <name>Estimate Noise Local (G-test)</name>
        <synopsis>Nonparametric Residual Noise Estimation using Local Gamma-test</synopsis>
        <help>
            It performs Gamma-test in the local range surrounding given sample. It first search for "range" nearest neighbors and then performs G-test among this "range" neighbors
        </help>         
        <icon>ispr.png</icon>
    </operator>
        
    <operator>
        <key>weightBy_Local_NNE_D_test</key>
        <name>Estimate Noise Local (D-test)</name>
        <synopsis>Nonparametric Residual Noise Estimation using Local Delta-test</synopsis>
        <help>
            It performs Delta-test in the local range surrounding given sample. It first search for "range" nearest neighbors and then performs D-test among this "range" neighbors
        </help>         
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>weightBy_LocalVariance_test</key>
        <name>Estimate Noise Local (Var.)</name>
        <synopsis>Nonparametric Residual Noise Estimation per example using local variance</synopsis>
        <help>            
        </help>         
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Flatten Clustering by Distance</name>
        <key>FlattenClustersByDistance</key>
        <!--     <class>org.prules.operator.clustering.FlattenByDistanceClusterModel</class>        -->
        <synopsis>Flatten Clusters by Distance value</synopsis>
        <help>This algorithm can be used as an alternative for default Flatten Clustering algorithm provided by RapidMiner.
            It is used to split data clustered with Agglomerative Clustering into clusters. &lt;br/&gt;
            Instead of manually setting the number of clusters this operator allows to define a threshold on the dendrogram height.
            It can be interpreted as a maximum allowed distance between different clusters. This could be visualized using dendrogram
            view of the Agglomerative Clustering operator as some cut-point level on which the dendrogram is catted. Unfortunately 
            the default RapidMiner Agglomerative Clustering operator don't provide a scale on the vertical axes of the dendrogram view.
        </help>
        <icon>ispr.png</icon>
    </operator>
    <!--
    <operator>
        <name>Flatten Clusters By Distance</name>
        <synopsis>Flatten Clusters By Distance</synopsis>
        <help>Similar to Flatten Clustering it cuts the dendrogram but instead of setting the number of clusters here it is based on setting the similarity threshold between clusters</help>
        <key>FlattenClustersByDistance</key>
    </operator>
    -->
    <operator>
        <name>Select by Random</name>
        <synopsis>Random instances selection</synopsis>
        <help>
            This operator performs naive example set filtering. It simple randomly selects examples.
            It is very similar to Sample operator.
        </help>
        <key>random_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Select by CNN</name>
        <synopsis>Condensed Neares Neighbor instance selection</synopsis>
        <help>
            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Hart P.E., The condensed nearest neighbor rule., IEEE Trans. on Information Theory, vol. 16, pp. 515-516, 1968 
        </help>
        <key>cnn_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Select by IB2</name>
        <synopsis>IB2 instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Acha D., Kibler D., Albert M, Instance-Based Learning Algorithms., Machine Learning, vol.6, pp. 37-66, 1991
        </help>
        <key>ib2_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by IB3</name>
        <synopsis>IB3 instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Acha D., Kibler D., Albert M, Instance-Based Learning Algorithms., Machine Learning, vol.6, pp. 37-66, 1991
        </help>
        <key>ib3_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by Drop1</name>
        <synopsis>Drop1 instance selection</synopsis>
        <help>   
            &lt;p&gt; Drop1 is an algorithm proposed by D. Randall Wilson, Tony R. Martinez in &lt;i&gt;Instance Pruning Techniques&lt;/i&gt; Machine Learning: Proceedings of the Fourteenth International Conference, Morgan Kaufmann Publishers, San Francisco, CA, pp. 404-411, 1997. &lt;/p&gt;
            &lt;p&gt; This algorithm removes an instance if its removal do not affect classification of other instances. For that purpose it calculates so called &lt;i&gt;improvement&lt;/i&gt; a measure which tells how many samples were classified correctly with an instance &lt;i&gt; p &lt;/i&gt; and without &lt;i&gt;p&lt;/i&gt;.
            If the result is positive instance is kept otherwise it is removed. &lt;/p&gt;      
        </help>
        <key>drop1_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by Drop2</name>
        <synopsis>Drop2 instance selection</synopsis>
        <help>     
            &lt;p&gt; Drop2 is an algorithm proposed by D. Randall Wilson, Tony R. Martinez in &lt;i&gt;Instance Pruning Techniques&lt;/i&gt; Machine Learning: Proceedings of the Fourteenth International Conference, Morgan Kaufmann Publishers, San Francisco, CA, pp. 404-411, 1997. &lt;/p&gt;
            &lt;p&gt; This algorithm removes an instance if its removal do not affect classification of other instances. For that purpose for each sample it calculates so called &lt;i&gt;improvement&lt;/i&gt; a measure which tells how many samples were classified correctly with an instance &lt;i&gt; p &lt;/i&gt; and without &lt;i&gt;p&lt;/i&gt;.
            If the result (with - without) is positive instance is kept otherwise it is removed. &lt;/p&gt;
            &lt;p&gt; Main difference to Drop1 is ordering samples according to nearest enemie - the farthest are considered first. It also uses entire training set to validate the effect of removal of sample &lt;i&gt;p&lt;/i&gt; while in Drop1 it uses only remaining samples after removal. &lt;/p&gt;    
        </help>
        <key>drop2_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by Drop3</name>
        <synopsis>Drop3 instance selection</synopsis>
        <help>         
            &lt;p&gt;Drop3 is an algorithm proposed by D. Randall Wilson, Tony R. Martinez in &lt;i&gt;Instance Pruning Techniques&lt;/i&gt; Machine Learning: Proceedings of the Fourteenth International Conference, Morgan Kaufmann Publishers, San Francisco, CA, pp. 404-411, 1997.
            &lt;p&gt;This algorithm removes an instance if its removal do not affect classification of other instances. For that purpose for each sample it calculates so called &lt;i&gt;improvement&lt;/i&gt; a measure which tells how many samples were classified correctly with an instance &lt;i&gt; p &lt;/i&gt; and without &lt;i&gt;p&lt;/i&gt;.
            If the result (with - without) is positive instance is kept otherwise it is removed.&lt;/p&gt;
            &lt;p&gt; Main difference to Drop2 is embedded  ENN instance selection, such that first the ENN is executed and afterwards the the  Drop2  &lt;/p&gt;
        </help>
        <key>drop3_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by Drop4</name>
        <synopsis>Drop4 instance selection</synopsis>
        <help>         
            &lt;p&gt; Drop4 is an algorithm proposed by D. Randall Wilson, Tony R. Martinez in &lt;i&gt;Reduction Techniques for Instance-Based Learning Algorithms&lt;/i&gt; Machine Learning, 38, 257–286, 2000 &lt;/p&gt;
            &lt;p&gt; This algorithm removes an instance if its removal do not affect classification of other instances. For that purpose for each sample it calculates so called &lt;i&gt;improvement&lt;/i&gt; a measure which tells how many samples were classified correctly with an instance &lt;i&gt; p &lt;/i&gt; and without &lt;i&gt;p&lt;/i&gt;. If the result (with - without) is positive instance is kept otherwise it is removed. &lt;/p&gt;
            &lt;p&gt; Main difference to Drop3 is embedded  ENN instance selection, but the instances removed by ENN are again validated checking if removal of &lt;i&gt;p&lt;/i&gt; do not affect classification of other instances.  afterwards the the  Drop2 is executed. &lt;/p&gt;
        </help>
        <key>drop4_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by Drop5</name>
        <synopsis>Drop5 instance selection</synopsis>
        <help>         
            &lt;p&gt;Drop5 is an algorithm proposed by D. Randall Wilson, Tony R. Martinez in &lt;i&gt;Reduction Techniques for Instance-Based Learning Algorithms&lt;/i&gt; Machine Learning, 38, 257–286, 2000 &lt;/p&gt;
            &lt;p&gt; This algorithm removes an instance if its removal do not affect classification of other instances. For that purpose for each sample it calculates so called &lt;i&gt;improvement&lt;/i&gt; a measure which tells how many samples were classified correctly with an instance &lt;i&gt; p &lt;/i&gt; and without &lt;i&gt;p&lt;/i&gt;. If the result (with - without) is positive instance is kept otherwise it is removed. &lt;/p&gt;
            &lt;p&gt; It is a modification of Drop2 where first the Drop2 is executed  on samples ordered from the nearest to the farthest enemies, then the order is reverted and the algorithm is re-executed until no further reduction is achieved.&lt;/p&gt;
        </help>
        <key>drop5_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by ICF</name>
        <synopsis>Inecremental Case Filtering (ICF) instance selection</synopsis>
        <help>         
            &lt;p&gt;ICF is an instance selectin algorithm developed by Brighton and Mellish in "Advances in instance selection for instance-based learning algorithms." Data mining and knowledge discovery 6.2 (2002).&lt;/p&gt;
            &lt;p&gt; It uses a rule which says that: Coverage must be &gt; Rehabilitanty . (for details see the above article). It starts performing ENN algorithm and than for each sample it checks the above rule.&lt;/p&gt;
        </help>
        <key>icf_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <name>Select by ENN</name>
        <synopsis>Edited Neares Neighbor instance selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Wilson D.L., Assymptotic properties of nearest neighbour rules using edited data., IEEE Trans. on Systems, Man, and Cybernetics, Computational Intelligence, SMC-2, pp. 408-421, 1972
        </help>
        <key>enn_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Select by RENN</name>
        <synopsis>Repeated Edited Neares Neighbor instances selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Tomek I., An experiment with the edited nearest-neighbor rule., IEEE Trans. on Systems, Man, and Cybernetics, vol. 6, pp. 448-452, 1976
        </help>
        <key>renn_sel</key>
        <icon>ispr.png</icon>
    </operator>
  
    <operator>
        <name>Select by All-kNN</name>
        <synopsis>All kNN instances selection</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Tomek I., An experiment with the edited nearest-neighbor rule., IEEE Trans. on Systems, Man, and Cybernetics, vol. 6, pp. 448-452, 1976
        </help>
        <key>all_knn_sel</key>
        <icon>ispr.png</icon>
    </operator>
	
    <operator>
        <name>Select by GE</name>
        <synopsis>Instance selection based on Gabriel Graph Editing. This algorithm keeps instances which are on the border between different classes according to Gabriel Graph </synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Binay K. Bhattacharya, Ronald S. Poulsen, Godfried T. Toussaint,  "Application of Proximity Graphs to Editing Nearest Neighbor Decision Rule",  International Symposium on Information Theory, Santa Monica, 1981.
            &lt;br/&gt;            
        </help>
        <key>ge_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by RNG</name>
        <synopsis>Instance selection based on Relative Neighbor Graph Editing This algorithm keeps instances which are on the border between different classes</synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
        &lt;br/&gt;
            Godfried T. Toussaint, The Relative Neighborhood Graph of a Finite Planar Set, Pattern Recognition, vol.12, No.4, pp.261-268, 1980
            &lt;br/&gt;            
        </help>
        <key>rng_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Select by RMHC</name>
        <synopsis>Instance selection based on Random Mutation Hill Climbing </synopsis>
        <help>            This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
            Skalak, David B. "Prototype and feature selection by sampling and random mutation hill climbing algorithms." Proceedings of the eleventh international conference on machine learning. 1994.
        &lt;br/&gt;
            
        </help>
        <key>rmhc_sel</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Select by MC</name>
        <synopsis>Instance selection based on Monte Carlo sampling </synopsis>
        <help> This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;                        
        </help>
        <key>mc_sel</key>
        <icon>ispr.png</icon>
    </operator>
    <!--
<operator>
    <name>IB3 selection</name>
    <synopsis>IB3 instances selection</synopsis>
    <help>IB3 selection</help>
    <key>ib3_sel</key>
</operator>  
    -->
    <operator>
        <name>Select by ELH</name>
        <synopsis>ELH instances selection</synopsis>
        <help> This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            For details of this particular algorithm see: 
            &lt;br/&gt;
            Cameron-Jones, R. M. "Instance selection by encoding length heuristic with random mutation hill climbing." Eighth Australian Joint Conference on Artificial Intelligence. 1995.
        </help>
        <key>elh_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by Weka IS</name>
        <key>weka_sel</key>
        <synopsis>Weka instances selection</synopsis>
        <help>             This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of any classification or regression task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            This operator is a wrapper over IS Library developed for Weka, and was implemented by Dr. Alvar Arnaiz-Gonzalez from University of Burgos
        </help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by Keel IS</name>
        <key>keel_sel</key>
        <synopsis>Keel instances selection</synopsis>
        <help>             This operator performs instance selection which is a process of selecting a subset of examples
            from the full training set of a classification task. &lt;br/&gt;
            As a result only a subset of instances is returned (they are returned on Prot output port).
             &lt;br/&gt;
            Originally this kind of algorithms were developed to improve the k-NN 
            algorithm to reduce computational complexity and reduce the influence of noise in the data.
             &lt;br/&gt;
            This operator is a wrapper over IS Library developed in Keel project, see: http://www.keel.es/            
        </help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by G-ENN</name>
        <synopsis>GENN instances selection</synopsis>
        <help>It is an implementation of Generalized ENN algorithm. It allows for replacing default kNN classifier by
            any other classification algorithm. By doing this user can tune training set for given particular classifier.
            By doing this all noisy examples will be filtered. However it may be very time consuming as it performs leave one out principle to perform such analysis 
        </help>
        <key>genn_sel</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Select by G-CNN</name>
        <synopsis>Generalized Condensed Neares Neighbor instance selection</synopsis>
        <help>It is an implementation of Generalized CNN algorithm. It allows for replacing default kNN classifier by
            any other classification algorithm. By doing this user can tune training set for given particular classifier.
                                    
        </help>
        <key>gcnn_sel</key>
        <icon>ispr.png</icon>
    </operator>
  
    <operator>
        <name>Rank Attributes (Infosel)</name>
        <synopsis>Rank attributes using Infosel++</synopsis>
        <help>Feature selection based on Infosel++ library, for detaitls see: www.prules.org. This library was written in C++, and this operator is 
            a wrapper for that library. It requires to define name of the feature selection algorithm and its parameters. 
            For estimation of probability dystribution it library performs internal discretization with one
            of build in methods. All the parameters are given as strings because this allows to keep this library open for new algorithms 
            implemented in it. &lt;/br&gt;
            Remember that this RapidMiner plugin requires to have infosel++.exe. It is not included in this extension. You have to ask for it Jacek Biesiada who recently works at  Cincinnati Children's Hospital Medical Center.
            &lt;/br&gt;
            When you have Infosel++.exe you have to configure this operator by pointing the folder which includes infosel++.exe library.
        </help>
        <key>infosel_featsel</key>
        <icon>ispr.png</icon>
    </operator>
    <!--
    <operator>
        <name>Feature weights transformer</name>
        <synopsis>Feature weights transformer</synopsis>
        <help>This operator is used to transform feature weights. After passing this transformer features which should be present 
            have weights equals 1, and features which should be removed have weights equal 0. 
        &lt;br/&gt;
            It was designed because Infosel++ returns feature order, such that 
            each feature has assigned natural number, and this number represents its importance. However these numbers are whithout 
            any information if they are ordered from the most relevant to the least one or in the oposit order. The order 
            depends on the algorithm. For example mi_mi retunrs higher values for more relevant features.                
        </help>
        <key>featsel_weightsmodifier</key>
    </operator>
    -->
    <!--
    <operator>
          <name>Feature Selection Operator</name>
          <synopsis>Feature Selection Operator</synopsis>
          <help>Feature Selection Operator</help>
          <class>org.prules.operator.learner.feature.selection.FeatureSelectionOperator</class>
          <key>featsel_model</key>
    </operator>
    -->
    <operator>
        <name>Fuzzy C-Means</name>        
        <synopsis>Fuzzy c-Means (FCM)</synopsis>
        <help>
            Fuzzy c-means clustering algorithm. As a result it aslo returns cluster centers on Proto output port. &lt;br/&gt;
            For details of this algorithm see: 
        &lt;br/&gt;        
            Bezdek J.C., Pattern Recognition with Fuzzy Objective Function Algorithms, Plenum, New York, 1981 
        </help>
        <!--      <class>org.prules.operator.learner.optimization.FCMOperator</class> -->
        <key>FCM</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Conditional Fuzzy C-Means</name>
        <synopsis>Conditional Fuzzy c-Means (CFCM)</synopsis>
        <help>Conditional Fuzzy c-means clustering algorithm. This algorithm requires weight attribute which is used as a clustering condition.
            As a result it aslo returns cluster centers on Proto output port. &lt;br/&gt;
            For details of this algorithm see: 
        &lt;br/&gt;
            Pedrycz W., Conditional Fuzzy C-Means, Pattern Recognition Letters, Vol. 17(6), 1996
        </help>
        <!--     <class>org.prules.operator.learner.optimization.CFCMOperator</class> -->
        <key>CFCM</key>
        <icon>ispr.png</icon>
    </operator>

    <operator>
        <name>Assign Labels</name>
        <synopsis>Label examples in prototype input based on labeled exampleset.</synopsis>
        <help>This operator determine class label for each example delivered to the Prototypes input based 
            on the frequency of class labels of the nearest instances from the ExampleSet input. 
            &lt;br/&gt;
            When clustering labeled data we often need label also the cluster centers, such that the prototypes could be used for example in classification problems. This operator labels cluster centers based on the data delivered to its ExampleSet input.
            It creates a Voronoi diagram for the dataset delivered to the prototype input, 
            then calculates some statistics for each cell based on data delivered 
            to the exampleSet input (examples of both prototapes and exampleSet must have common attribute space)
            From this statisitcs it takes the most frequent class label in each cell and set it to the prototype representing given cell.
        &lt;br/&gt;
            Usually it is used for assigning label to the prototypes (cluster centers) obtained after clustering some labeled exampleset.            
        </help>
        <!--     <class>org.prules.operator.learner.misc.ClassAssignerOperator</class> -->
        <key>ClassAssigner</key>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <name>Loop Label Values</name>
        <synopsis>Iterates over each class label of an example set </synopsis>
        <help> This operator iterates over each value of class label of input example set, allowing to applay 
            instance processing method (ex. clastering) to each class independent. Finally all example sets delivered 
            to the output are combined such that the output Prototypes containes concatenation of all instances delivered to the internal output.
            If attribute subset is not consistent then output set contains all possible attributes, and missing values are set to NaN.
            &lt;br/&gt;
            For example it can be used to assign class labels to prototypes (cluster centers) 
            obtained as a result of clustering algorithm, or
            when having a labeled dataset and someone wants to cluster examples separately for each class label. (labels must be nominal)          
        </help>
        <!--     <class>org.prules.operator.learner.misc.ClassIteratorOperator</class>-->
        <key>ClassIterator</key>
        <icon>ispr.png</icon>
    </operator>  
    
    <operator>
        <name>Weight Examples by ENN</name>
        <synopsis>Instance Weighting</synopsis>
        <help>    
            This algorithm performs ENN algorithm but instead of selecting instances it assignes weight to each instance. 
            This weight is in range [0,1], and represent the value which can be interpreted as a probability of not being an outlier or noise sample.
            Among other usage, it can be used to train WLVQ network.
        </help>
        <!--     <class>org.prules.operator.learner.weighting.ENNWeightingOperator</class> -->
        <key>ENNWeighting</key>
        <icon>ispr.png</icon>
    </operator> 
    <!--
    <operator>
        <name>Weight Transformation</name>
        <synopsis>Weight Transformation</synopsis>
        <help> 
        </help>
        <class>org.prules.operator.learner.weighting.AttributeTransformation</class>
        <key>WeightTransformation</key>
    </operator> 
    -->
    <!--
    <operator>
        <name>Values Difference</name>
        <synopsis>For selected attributes it calculates a difference between vollowing neighbor values</synopsis>
        <help> 
        </help>
        <class>org.prules.operator.learner.misc.ValueDifferenceOperator</class>
        <key>ValueDifference</key>
    </operator>       
    <operator>
        <name>Values Shift</name>
        <synopsis>For selected attributes it shifts values from previous to new position</synopsis>
        <help> 
        </help>
        <class>org.prules.operator.learner.misc.ValuesShiftOperator</class>
        <key>ValuesShift</key>
    </operator>       
    -->
    <operator>
        <name>Export Process</name>
        <synopsis>Export the process to a file or a repository</synopsis>
        <help> 
            This operator makes a copy of the xml process description to a given location. It is a specialy usefull when you run multiple experiments with different modifications, so you can store not only the results log but automatically the corresponding process description. So it is simmilar to Auto-Save As where you can specify the location of the process.
        </help>
        <!--     <class>org.prules.operator.learner.misc.ProcessExportOperator</class>                -->
        <key>ExportProcess</key>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <name>Group Distance</name>
        <synopsis>Calculate the distance between group of examples</synopsis>
        <help> 
            This operator splits the input exampleSet according to the "grouping attribute", then it calculates the distance between each of the groups. The
            distance between groups is calculated as the average of the distance between two closes examples in each group. The returned exampleset is a square matrix 
            however it is not symetric, because it is calculated ina way where it takes each exampe from one group and finds closest example in the second group. So it
            is possible that group A will be mre similar to group B then group B is similar to group A. For example it may happen when examples in group A are a
            subset of examples in group B.
        </help>
        <!--     <class>org.prules.operator.learner.misc.GroupDistanceOperator</class>                -->
        <key>GroupDistance</key>
        <icon>ispr.png</icon>
    </operator> 
    <operator>
        <key>SubtractExampleSets</key>
        <name>Subtract ExampleSets</name>
        <help>This operator allows to subtract samples from one example set from the other such that only the remaining samples are left. In oposite to default operator it compares values of the examples not ID attribte</help>
    </operator>
    <operator>
        <synopsis>Delivers as logging value an input object ID</synopsis>
        <help> 
            This operator can be used to provide as a log value the ID value of the input IOObject (this could be ExampleSet or Prediction mMopdel or any other operator).
            
            In can be used to identify the model, which returned certain results. For example when using X-Validation (parallel )  it is impossible to combine results logged on the training and test side. By using prediction model ID you can collect logs independently on the training side and testing side and then join them together.
        </help>
        <name>Get IOObject ID</name>
        <key>InputID</key>
        <icon>ispr.png</icon>
        <!--     <class>org.prules.operator.learner.misc.GetIOModelID</class>                -->
    </operator> 
    <operator>
        <name>Performance IS</name>
        <key>is_performance</key>
        <!--     <class>org.prules.operator.performance.InstanceSSelectionPerformance</class>        -->
        <synopsis>Calculate various IS performances</synopsis>
        <help> 
            This operator gets two ExampleSet's as input and calculates various statistics related to the instance selection problems. These are size_difference - a difference between sizes of both example sets (the value is abs), relative_size_difference - size_difference divided by the size of the first example set, compression - calculated as the ration of example set sizes
        </help>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <name>Performance IS (Clustering)</name>
        <key>cluster_performance</key>
        <!--     <class>org.prules.operator.performance.ClusteringMinimumVarianceCriterion</class>        -->
        <synopsis>Calculate within cluster variance and returns it as performance</synopsis>
        <help> 
            This operator is used to analyze the performance of the clustering methods. It gets a set of cluster prototypes and  ExampleSet's as input, and calculates the variance within clusters - the same as is used in k-means clustering
        </help>
        <icon>ispr.png</icon>
    </operator>  
    <operator>
        <key>bagging_sel</key>
        <name>Ensemble IS by Bagging</name>
        <!--     <class>org.prules.operator.learner.selection.meta.ISMetaBaggingOperator</class> -->
        <synopsis>Ensemble Instance Selection based on Bagging</synopsis>
        <help>This operator implements bagging instance selection. It repeates instance selectin process given number of times and for each example calculates average number of times example was selected. The number of times given vector was selected depends on the ID in the output set of the subprocess. So if all vectors will be delivered to the output all vectors will be selected (according to the bootstrap distribution)</help>
        <icon>ispr.png</icon>
    </operator>
    <operator>
        <key>adaboost_sel</key>
        <name>Ensemble IS by AdaBoost</name>        
        <synopsis>Ensemble Instance Selection based on AdaBoost</synopsis>
        <help>This operator implements AdaBoost instance selection. It repeates instance selectin process given number of times and for each example calculates average number of times example was selected preserving weights generated by AdaBoost. The number of times given vector was selected depends on the ID in the output set of the subprocess. So if all vectors will be delivered to the output all vectors will be selected (according to the bootstrap distribution)</help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>vote_sel</key>
        <name>Ensemble IS by Vote</name>
        <!--  <class>org.prules.operator.learner.selection.meta.ISMetaVoteOperator</class> -->
        <synopsis>Ensemble Instance Selection based on Voting</synopsis>
        <help>It allows to combine results of many independent instance selection algorithms. It combines selected instances and calculate votes, how often given example was selected by all of the instance selection methods. Finally instances selected more often then the threshold will appear in the final selected example set.</help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>RndFeature_sel</key>
        <name>Ensemble IS by Attribute Subsets</name>
        <!--  <class>org.prules.operator.learner.selection.meta.ISMetaRndFeatureOperator</class>-->
        <synopsis>Ensemble Instance Selection based on Random Features</synopsis>
        <help>
            This meta operator for instance selection utilize the random features principle to support divergence of the model.
            This algorithm repeats instance selection process, and in each iteration a subset of features (ratio defines the percentage of features to be select) is selected  and delivered to the input of the process. According to the selected instances  in each iteration a ranking is made, such that the weight represents the number of times given instance was returned by the instance selection subprocess. The threshold parameter allows to manipulate the popularity of selected instances. If the "return weight" option is selected instead of selecting the instances a weight attribute is returned. This operator requires ExampleSet with ID attribute to identify particular instance.
        </help>
        <icon>ispr.png</icon>
    </operator>   
        
    <operator>
        <key>meta_noise_sel</key>
        <name>Ensemble IS by Noise</name>
        <!-- <class>org.prules.operator.learner.selection.meta.ISMetaNoisOperator</class> -->
        <synopsis>Ensemble Instance Selection based on additive Gaussian Noise</synopsis>
        <help>      
            This meta operator for instance selection utilize the random features principle to support divergence of the model.
            This algorithm repeats instance selection process, and in each iteration a subset of features (ratio defines the percentage of features to be select) is selected  and delivered to the input of the process. According to the selected instances  in each iteration a ranking is made, such that the weight represents the number of times given instance was returned by the instance selection subprocess. The threshold parameter allows to manipulate the popularity of selected instances. If the "return weight" option is selected instead of selecting the instances a weight attribute is returned. This operator requires ExampleSet with ID attribute to identify particular instance.
        </help>
        <icon>ispr.png</icon>
    </operator>   
    
    <!--
<key>mds_scaling</key>
<class>org.prules.operator.learner.feature.construction.MdsOperator</class>
    -->			
    <operator>
        <key>mds_scaling</key>
        <name>Multidimensional Scalling</name>
        <snortname>MDS</snortname>
        <synopsis>This operator uses special MDS algorithm from MDSJ library to provide a visual
            representation of the pattern of proximities (i.e., similarities or
            distances) among a set of objects.
        </synopsis>
        <help>An MDS algorithm aims to place an example defined in high dimensional space N > 2 into N'-dimensional space (usually N'=2) such that the between-object distances in the high dimensional (origininal)  space and in the low dimensional space are preserved as accurate as possible.&lt;br&gt; Note that MDSJ library used by the operator is licenced under the Creative Commons License &quot;by-nc-sa&quot; 3.0 regulations. This means that you can: &lt;ul&gt; &lt;li&gt;freely copy, share, and distribute the library at no cost and &lt;/li&gt; &lt;li&gt;re-use the library as a component in your software,&lt;/li&gt; &lt;/ul&gt; as long as you: &lt;ul&gt; &lt;li&gt; include the citation below (by), &lt;/li&gt; &lt;li&gt; do not use MDSJ for any commercial purposes (nc), and &lt;/li&gt; &lt;li&gt; apply these conditions to all your pieces of software that make use of MDSJ (sa).&lt;/li&gt; &lt;/ul&gt; . &lt;br&gt; Citation: &quot;Algorithmics Group. MDSJ: Java Library for Multidimensional Scaling (Version 0.2). Available at http://www.inf.uni-konstanz.de/algo/software/mdsj/. University of Konstanz, 2009.&quot;
        </help>
        <icon>ispr.png</icon>
    </operator>
    
    <operator>
        <key>VDMNominal2NumericalOperator</key>
        <name>Nominal to Numerical VDM</name>                
        <synopsis>Converts selected nominal attributes to numerical attributes using Value Divverenc Matric (VDM) procedure</synopsis>
        <help>
            <![CDATA[ 
                <p>
This operator is applicable to all classification problem to convert nominal attributes to numerical. It is much more powerful then the dummy codding available in RapidMiner. <br>
It uses Value Difference Metric (VDM) where nominal attribute is transformed into c numerical attributes where c is the number of class labels. In VDM first the probabilities Psc=Nsc/Ns are calculated where Psc is the probability of belonging of symbol s to class c, and Ns is the number of times symbol s appeared in the training set, and Nsc is the number of times symbol s appeared in association with class c. Then, every occurrence of the symbol s of given attribute is encoded as c values [Psc1, Psc2, …,Psc3] respectively for each class label. Finally, when distance between two objects needs to be calculated standard Euclidean distance can be calculated between Psc values. </p>
<p>
So finally we obtain distance between A,B d(A,B) =>
</p>
<p>
d(A,B)^2 = sum_i=1:c(  A_Psci – B_Psci)^2
</p>
<p>
Standard usage scenario includes: 
<ol>
	<li>
		Numerical attributes normalization (usually in range [0-1])
	</li>
	<li>
		The VDM transformation of all symbolic attributes, what result in pure numerical dataset.
	</li>
	<li>
		Prediction model training and application
	</li>
</ol>
</p>
<p>
Note that VDM transformation extensively uses class labels information, so remember to apply it on the test set, and in validation it should be applied internally – inside cross-validation, otherwise results would be overestimated
</p>
            ]]>
        </help>
    </operator>  
    
    <!-- This is how group keys are translated: -->
    <group>
        <key>Feature_reduction</key>
        <name>Feature reduction</name>
    </group>
</operatorHelp>
